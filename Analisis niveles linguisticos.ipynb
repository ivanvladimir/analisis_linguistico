{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7a934a-b7a9-4532-95ca-3f025fb74b5d",
   "metadata": {},
   "source": [
    "# Análisis niveles lingüisticos\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ivanvladimir/analisis_linguistico/blob/main/Analisis%20niveles%20linguisticos.ipynb)\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/ivanvladimir/analisis_linguistico/blob/main/Analisis%20niveles%20linguisticos.ipynb)\n",
    "\n",
    "Este es el código para ejemplificar análisis computacional lingüístico: diferentes niveles\n",
    "\n",
    "### Instrucciones\n",
    "\n",
    "Ejecutar las celdas en el orden que se encuentran.\n",
    "\n",
    "### Licencia de la notebook\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.\n",
    "\n",
    "### Información general\n",
    "\n",
    "> **Author(s)**: <a href=\"https://twitter.com/ivanvladimir\">@ivanvladimir</a> </br>\n",
    "> **Last updated**: 15/06/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eccd19-cc2e-42a7-8a5e-1f2e4c515cbc",
   "metadata": {},
   "source": [
    "# ❶  Preparar librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dccf21-e9f3-4c75-a2d8-829399648494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar librerias\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8fd58-f1f4-4a6d-9116-fe871a9fd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerias\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import os\n",
    "import random\n",
    "from stanza import DownloadMethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "import networkx as nx\n",
    "from typing import List, Tuple, Dict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa448d-1eec-47d7-b72a-0fb81883aab6",
   "metadata": {},
   "source": [
    "# ❷ Preparar datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc129b5-38bd-4147-82e3-4f1cd53acb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bajar datos mañanera\n",
    "!git clone https://github.com/NOSTRODATA/conferencias_matutinas_amlo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18dbd2-eeb1-4546-8777-0c4c152b2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poner todos los datos en un dataframe\n",
    "\n",
    "dataframes=[]\n",
    "\n",
    "for root, dirs, files in os.walk(\"conferencias_matutinas_amlo/\", topdown=False):\n",
    "   for name in files:\n",
    "      if name.startswith('mananera') and name.endswith(\".csv\"):\n",
    "        try:\n",
    "            filename=os.path.join(root,name)\n",
    "            df = pd.read_csv(filename)\n",
    "            df['source_file'] = filename\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "try:\n",
    "    df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining dataframes: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1b5b1-e120-4b5a-abc1-ff299e1ce309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f39c2-6639-4ad0-97fa-c8aeaf264eb2",
   "metadata": {},
   "source": [
    "# ❸ Escoger aleatoriamente un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ec91d-8c2f-4767-8742-8b09de090b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "amlo_df = df[df['Participante']=='PRESIDENTE ANDRES MANUEL LOPEZ OBRADOR']\n",
    "indice_texto = random.randint(0,len(amlo_df))\n",
    "text=amlo_df.iloc[indice_texto]['Texto'] # <--- Cambiar el índice para escoger otro texto\n",
    "\n",
    "print(f\"Texto a analizar [{indice_texto}]: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578dc7d-1d75-49fd-8229-68c32fe2a1e9",
   "metadata": {},
   "source": [
    "# ❹ Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eedc6-daa0-4cde-b75b-85e2c31e4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para más información ver: https://stanfordnlp.github.io/stanza/tokenize.html\n",
    "\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize',download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "doc = nlp(text)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7522212-c468-4bc3-abdb-e5fa823c44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando Multi-palabra token\n",
    "\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt',download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "doc = nlp(text)\n",
    "for i,sent in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    for token in sent.tokens:\n",
    "        print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ae2d9-0855-46fc-bd65-ffa80f2c07a7",
   "metadata": {},
   "source": [
    "# ❺ Etiquetación de partes del habla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdd1fa-6897-4b69-b603-1d95bf2c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver más: https://stanfordnlp.github.io/stanza/pos.html\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos')\n",
    "doc = nlp(text)\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288e96f-fbff-43d4-9c62-eda66ff72c91",
   "metadata": {},
   "source": [
    "# ❻ Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee8dcc-9517-4073-af50-39123ec54cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver más: https://stanfordnlp.github.io/stanza/lemma.html\n",
    "\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "doc = nlp(text)\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ddfafe-3c0e-4c5d-a7aa-3c651462c54d",
   "metadata": {},
   "source": [
    "# ❼ Árbol de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6040d-70a0-45dc-9fd0-d55f6e2c3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver más en: https://stanfordnlp.github.io/stanza/depparse.html\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "doc = nlp(text)\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d23f89-a99a-49df-bd3c-c5565a708d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dependencies(doc: stanza.Document) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract dependency information from parsed document.\n",
    "    \n",
    "    Args:\n",
    "        doc (stanza.Document): Parsed Stanza document\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of sentences with dependency information\n",
    "    \"\"\"\n",
    "    sentences_data = []\n",
    "    \n",
    "    for sent_idx, sent in enumerate(doc.sentences):\n",
    "        words = []\n",
    "        dependencies = []\n",
    "        \n",
    "        for word in sent.words:\n",
    "            words.append({\n",
    "                'id': word.id,\n",
    "                'text': word.text,\n",
    "                'lemma': word.lemma,\n",
    "                'pos': word.upos,\n",
    "                'xpos': word.xpos,\n",
    "                'head': word.head,\n",
    "                'deprel': word.deprel\n",
    "            })\n",
    "            \n",
    "            # Add dependency relation (skip root)\n",
    "            if word.head != 0:\n",
    "                dependencies.append((word.head, word.id, word.deprel))\n",
    "        \n",
    "        sentences_data.append({\n",
    "            'sentence_idx': sent_idx,\n",
    "            'text': sent.text,\n",
    "            'words': words,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    \n",
    "    return sentences_data\n",
    "\n",
    "def visualize_dependency_tree(doc, sentence_idx=0):\n",
    "    \"\"\"Visualize dependency tree using NetworkX\"\"\"\n",
    "    sent = doc.sentences[sentence_idx]\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for word in sent.words:\n",
    "        G.add_node(word.id, label=f\"{word.text}\\n({word.pos})\")\n",
    "        if word.head != 0:  # Not root\n",
    "            G.add_edge(word.head, word.id, label=word.deprel)\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    # Draw graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                          node_size=2000, alpha=0.7)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                          arrows=True, arrowsize=20, arrowstyle='->')\n",
    "    \n",
    "    # Add node labels\n",
    "    node_labels = nx.get_node_attributes(G, 'label')\n",
    "    nx.draw_networkx_labels(G, pos, node_labels, font_size=8)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6)\n",
    "    \n",
    "    plt.title(f\"Dependency Tree: {' '.join([w.text for w in sent.words])}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_tree_matplotlib(doc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87885b00-5de6-41fc-a93e-0fa44611d1d2",
   "metadata": {},
   "source": [
    "# ❽ Reconocimiento de entidades nombradas y más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4081ae0-a797-45eb-a1b8-03ed77dd15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,ner')\n",
    "doc = nlp(text)\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8befc0f-40cd-464a-90b0-575c1db42878",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "doc = nlp(text)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(\"%d -> %s\" % (i, \"positivo\" if sentence.sentiment == 1 else \"negativo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ff543-f725-4453-8a15-bc07c5191a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
